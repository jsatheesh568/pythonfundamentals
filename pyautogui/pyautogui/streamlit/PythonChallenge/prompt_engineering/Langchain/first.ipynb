{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26f5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = PyPDFLoader('sample.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c81bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First attempt without retrieval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsath\\AppData\\Local\\Temp\\ipykernel_13428\\2697269739.py:38: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  first_answer = llm.predict(f\"Q: {question}\\nA:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First answer: The capital of France is Paris.\n",
      "\n",
      "Final answer: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env file if you use .env (optional)\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Load & split\n",
    "loader = TextLoader(\"notes.txt\")\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings + vectorstore\n",
    "# <-- no key in code if you use env var\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Retriever & QA chain\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Pass key here only if you didn't set env var\n",
    "llm = ChatOpenAI(temperature=0)  \n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "def self_rag_query(question):\n",
    "    print(\"First attempt without retrieval\")\n",
    "    # simple way to get answer text (langchain versions differ; 'predict' is simpler)\n",
    "    try:\n",
    "        first_answer = llm.predict(f\"Q: {question}\\nA:\")\n",
    "    except AttributeError:\n",
    "        # fallback if predict not available in your version\n",
    "        raw = llm.invoke([HumanMessage(content=f\"Q: {question}\\nA:\")])\n",
    "        first_answer = raw.content if hasattr(raw, \"content\") else str(raw)\n",
    "\n",
    "    print(\"First answer:\", first_answer)\n",
    "\n",
    "    if \"I'm not sure\" in first_answer or len(first_answer.strip()) < 30:\n",
    "        print(\"Low confidence. Retrieving context and trying again...\")\n",
    "        improved_answer = qa.run(question)\n",
    "        return improved_answer\n",
    "    else:\n",
    "        return first_answer\n",
    "\n",
    "response = self_rag_query(\"What is the capital of France?\")\n",
    "print(\"\\nFinal answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5026a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env file if you use .env (optional)\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load & split\n",
    "loader = TextLoader(\"notes.txt\")\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Embeddings + vectorstore\n",
    "# <-- no key in code if you use env var\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Retriever & QA chain\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Pass key here only if you didn't set env var\n",
    "llm = ChatOpenAI(temperature=0)  \n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "def corrective_rag(question):\n",
    "\tfirst_guess =llm.predict (f\"Try to answer : {question}\")\n",
    "\tdocs = [Document(page_content =\"The largest cat is a tiger.\")]\n",
    "\tdb =FAISS.from_documents(docs, embeddings)\n",
    "\t\n",
    "\tqa_chain = RetrievalQA.from_chain_type(llm=llm , retriever =db.as_retriever())\n",
    "\tcorrection = qa_chain.run(question)\n",
    "\t\n",
    "\treturn f\"Original answer :{first_guess}\\n Corrected using documents: {correction}\"\n",
    "\t\n",
    "\tprint(corrective_rag(\"what is the¬†largest¬†cat?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7490119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the largest cat?\n",
      "Running corrective RAG...\n",
      "\n",
      "DEBUG - raw_evaluation:\n",
      " {\n",
      "  \"relevance\": 0.0,\n",
      "  \"completeness\": 0.0,\n",
      "  \"accuracy\": 0.0,\n",
      "  \"specificity\": 0.0,\n",
      "  \"overall\": \"POOR\",\n",
      "  \"justification\": \"The retrieved context is completely irrelevant to the query.\",\n",
      "  \"decision\": {\n",
      "     \"action\": \"RETRIEVE_AGAIN\",\n",
      "     \"new_query\": \"\",\n",
      "     \"reasoning\": \"The current retrieved context does not provide any information related to the query.\",\n",
      "     \"confidence\": \"low\"\n",
      "  }\n",
      "}\n",
      "üîç Context Quality: POOR\n",
      "Relevance Score: 0.0\n",
      "Completeness Score: 0.0\n",
      "Accuracy Score: 0.0\n",
      "Specificity Score: 0.0\n",
      "Justification: The retrieved context is completely irrelevant to the query.\n",
      "\n",
      "ACTION: RETRIEVE_AGAIN\n",
      "NEW_QUERY: largest feline species size characteristics\n",
      "REASONING: The current retrieved context does not provide any information related to the query.\n",
      "\n",
      "Corrected Answer (from re-retrieved docs):\n",
      "The largest cat is the Siberian tiger, also known as the Amur tiger. They can weigh up to 660 pounds and grow up to 11 feet in length, including their tail.\n",
      "\n",
      "SOURCES:\n",
      "- (len=49) {'source': 'notes.txt'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env file if you use .env (optional)\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ---------------------------\n",
    "# Config / API key handling\n",
    "# ---------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. Please set it in your environment or in a .env file.\\n\\n\"\n",
    "        \"Example .env content:\\n\"\n",
    "        \"  OPENAI_API_KEY=sk-your_api_key_here\\n\"\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Corrective RAG System Prompt (JSON output requested)\n",
    "# Note: all literal braces are doubled so .format() won't treat them as placeholders,\n",
    "# except {user_query} and {retrieved_context} which we intend to replace.\n",
    "# ---------------------------\n",
    "CORRECTIVE_RAG_SYSTEM_PROMPT = \"\"\"\n",
    "You are a Corrective RAG system that evaluates retrieved context quality and corrects retrieval when necessary.\n",
    "\n",
    "Primary Workflow:\n",
    "\n",
    "Step 1: Context Evaluation\n",
    "EVALUATE_CONTEXT: Rate the following retrieved context for the given query.\n",
    "\n",
    "Query: {user_query}\n",
    "Retrieved Context: {retrieved_context}\n",
    "\n",
    "Return a JSON object EXACTLY with these fields:\n",
    "{{\n",
    "  \"relevance\": <float 0.0-1.0>,\n",
    "  \"completeness\": <float 0.0-1.0>,\n",
    "  \"accuracy\": <float 0.0-1.0>,\n",
    "  \"specificity\": <float 0.0-1.0>,\n",
    "  \"overall\": \"<EXCELLENT|GOOD|FAIR|POOR>\",\n",
    "  \"justification\": \"<one-sentence justification>\",\n",
    "  \"decision\": {{\n",
    "     \"action\": \"<RETRIEVE_AGAIN|PROCEED_WITH_ANSWER>\",\n",
    "     \"new_query\": \"<refined_query or empty string>\",\n",
    "     \"reasoning\": \"<short reason>\",\n",
    "     \"confidence\": \"<high|medium|low or empty>\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Important:\n",
    "- All numeric scores must be between 0.0 and 1.0.\n",
    "- \"overall\" must be one of: EXCELLENT, GOOD, FAIR, POOR.\n",
    "- Fill \"new_query\" only when action == \"RETRIEVE_AGAIN\" (otherwise empty string).\n",
    "- Return ONLY the JSON object (no extra commentary).\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Setup: load docs, embeddings, vectorstore\n",
    "# ---------------------------\n",
    "loader = TextLoader(\"notes.txt\")  # make sure this file exists\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: extract JSON substring\n",
    "# ---------------------------\n",
    "def extract_json_substring(s: str) -> str:\n",
    "    brace_stack = []\n",
    "    start_idx = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch == \"{\":\n",
    "            if start_idx is None:\n",
    "                start_idx = i\n",
    "            brace_stack.append(i)\n",
    "        elif ch == \"}\":\n",
    "            if brace_stack:\n",
    "                brace_stack.pop()\n",
    "                if not brace_stack and start_idx is not None:\n",
    "                    return s[start_idx:i + 1]\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Defensive evaluation function\n",
    "# ---------------------------\n",
    "def evaluate_retrieved_context_json(user_query: str, retrieved_context: str) -> dict:\n",
    "    prompt = CORRECTIVE_RAG_SYSTEM_PROMPT.format(\n",
    "        user_query=user_query.replace('\"', \"'\"),\n",
    "        retrieved_context=retrieved_context.replace('\"', \"'\")\n",
    "    ) + \"\\n\\nReturn the JSON now.\"\n",
    "\n",
    "    try:\n",
    "        raw = llm.predict(prompt)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"relevance\": 0.0,\n",
    "            \"completeness\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"specificity\": 0.0,\n",
    "            \"overall\": \"POOR\",\n",
    "            \"justification\": f\"LLM call failed: {str(e)}\",\n",
    "            \"raw_evaluation\": \"\",\n",
    "            \"decision\": {\"action\": \"RETRIEVE_AGAIN\", \"new_query\": \"\", \"reasoning\": \"LLM call error\", \"confidence\": \"\"}\n",
    "        }\n",
    "\n",
    "    parsed = None\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except Exception:\n",
    "        candidate = extract_json_substring(raw)\n",
    "        if candidate:\n",
    "            try:\n",
    "                parsed = json.loads(candidate)\n",
    "            except Exception:\n",
    "                parsed = None\n",
    "\n",
    "    if not parsed or not isinstance(parsed, dict):\n",
    "        return {\n",
    "            \"relevance\": 0.0,\n",
    "            \"completeness\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"specificity\": 0.0,\n",
    "            \"overall\": \"POOR\",\n",
    "            \"justification\": \"Evaluation JSON parse failed. See raw_evaluation for details.\",\n",
    "            \"raw_evaluation\": raw,\n",
    "            \"decision\": {\"action\": \"RETRIEVE_AGAIN\", \"new_query\": \"\", \"reasoning\": \"Parsing failure\", \"confidence\": \"\"}\n",
    "        }\n",
    "\n",
    "    safe = {}\n",
    "    for key in (\"relevance\", \"completeness\", \"accuracy\", \"specificity\"):\n",
    "        try:\n",
    "            val = float(parsed.get(key, 0.0))\n",
    "            safe[key] = max(0.0, min(1.0, val))\n",
    "        except Exception:\n",
    "            safe[key] = 0.0\n",
    "\n",
    "    overall = str(parsed.get(\"overall\", \"\")).upper()\n",
    "    safe[\"overall\"] = overall if overall in (\"EXCELLENT\", \"GOOD\", \"FAIR\", \"POOR\") else \"POOR\"\n",
    "    safe[\"justification\"] = str(parsed.get(\"justification\", \"\")) or \"\"\n",
    "    decision = parsed.get(\"decision\", {}) or {}\n",
    "    safe[\"decision\"] = {\n",
    "        \"action\": str(decision.get(\"action\", \"RETRIEVE_AGAIN\")).upper(),\n",
    "        \"new_query\": str(decision.get(\"new_query\", \"\") or \"\"),\n",
    "        \"reasoning\": str(decision.get(\"reasoning\", \"\") or \"\"),\n",
    "        \"confidence\": str(decision.get(\"confidence\", \"\") or \"\")\n",
    "    }\n",
    "    safe[\"raw_evaluation\"] = parsed.get(\"raw_evaluation\", \"\") or raw\n",
    "    return safe\n",
    "\n",
    "# ---------------------------\n",
    "# Refined-query helper\n",
    "# ---------------------------\n",
    "def generate_refined_query(user_query: str, retrieved_context: str) -> str:\n",
    "    refine_prompt = (\n",
    "        \"You are a query-refinement assistant. Produce a short keyword-focused query (one line) \"\n",
    "        \"that will retrieve more relevant documents for the user's intent.\\n\\n\"\n",
    "        f\"User query: {user_query}\\n\\nRetrieved context:\\n{retrieved_context}\\n\\nRefined query:\"\n",
    "    )\n",
    "    try:\n",
    "        out = llm.predict(refine_prompt)\n",
    "        return out.splitlines()[0].strip()\n",
    "    except Exception:\n",
    "        return user_query\n",
    "\n",
    "# ---------------------------\n",
    "# Helper to run QA chain and extract result + sources\n",
    "# ---------------------------\n",
    "def run_qa_chain(qa_chain, question: str):\n",
    "    \"\"\"\n",
    "    Invoke the chain safely and return tuple (answer_text, source_documents_list)\n",
    "    \"\"\"\n",
    "    # Newer LangChain chains accept a dict input and return a dict of outputs\n",
    "    outputs = qa_chain.invoke({\"query\": question})\n",
    "    # fallback to calling chain as callable if invoke not present\n",
    "    if outputs is None:\n",
    "        outputs = qa_chain({\"query\": question})\n",
    "\n",
    "    # outputs is a dict; get the main result\n",
    "    answer = outputs.get(\"result\") or outputs.get(\"output_text\") or outputs.get(\"answer\") or \"\"\n",
    "    sources = outputs.get(\"source_documents\") or outputs.get(\"source_documents\", []) or []\n",
    "    return answer, sources\n",
    "\n",
    "# ---------------------------\n",
    "# Main corrective_rag function (fixed to use run_qa_chain)\n",
    "# ---------------------------\n",
    "def corrective_rag(question: str, debug=False) -> str:\n",
    "    hits = retriever.get_relevant_documents(question)\n",
    "    if not hits:\n",
    "        return \"No documents retrieved to evaluate.\"\n",
    "\n",
    "    top_context = \"\\n\\n---\\n\\n\".join([d.page_content for d in hits[:1]])\n",
    "    eval_json = evaluate_retrieved_context_json(question, top_context)\n",
    "\n",
    "    if debug and eval_json.get(\"raw_evaluation\"):\n",
    "        print(\"DEBUG - raw_evaluation:\\n\", eval_json.get(\"raw_evaluation\"))\n",
    "\n",
    "    overall = (eval_json.get(\"overall\") or \"POOR\").upper()\n",
    "    relevance = eval_json.get(\"relevance\", 0.0)\n",
    "    completeness = eval_json.get(\"completeness\", 0.0)\n",
    "    accuracy = eval_json.get(\"accuracy\", 0.0)\n",
    "    specificity = eval_json.get(\"specificity\", 0.0)\n",
    "    justification = eval_json.get(\"justification\", \"\")\n",
    "\n",
    "    eval_summary_lines = [\n",
    "        f\"üîç Context Quality: {overall}\",\n",
    "        f\"Relevance Score: {relevance}\",\n",
    "        f\"Completeness Score: {completeness}\",\n",
    "        f\"Accuracy Score: {accuracy}\",\n",
    "        f\"Specificity Score: {specificity}\",\n",
    "        f\"Justification: {justification}\",\n",
    "    ]\n",
    "    eval_block = \"\\n\".join(eval_summary_lines)\n",
    "\n",
    "    decision = eval_json.get(\"decision\", {}) or {}\n",
    "    action = (decision.get(\"action\") or \"RETRIEVE_AGAIN\").upper()\n",
    "    refined_query_from_decision = decision.get(\"new_query\") or \"\"\n",
    "    reasoning = decision.get(\"reasoning\") or \"\"\n",
    "\n",
    "    if action == \"RETRIEVE_AGAIN\" or overall in (\"POOR\", \"FAIR\"):\n",
    "        refined_query = refined_query_from_decision or generate_refined_query(question, top_context)\n",
    "        reasoning = reasoning or \"Retrieved context rated low ‚Äî re-retrieving with refined query.\"\n",
    "\n",
    "        new_hits = retriever.get_relevant_documents(refined_query)\n",
    "        if not new_hits:\n",
    "            return (\n",
    "                eval_block\n",
    "                + \"\\n\\nACTION: RETRIEVE_AGAIN\\nNEW_QUERY: \"\n",
    "                + refined_query\n",
    "                + \"\\nREASONING: \"\n",
    "                + reasoning\n",
    "                + \"\\n\\nNo documents found for the refined query.\"\n",
    "            )\n",
    "\n",
    "        db = FAISS.from_documents(new_hits, embeddings)\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(), return_source_documents=True)\n",
    "\n",
    "        # Use the safe runner\n",
    "        final_answer, sources = run_qa_chain(qa_chain, question)\n",
    "\n",
    "        # Optionally format sources for display\n",
    "        sources_text = \"\"\n",
    "        if sources:\n",
    "            sources_text = \"\\n\\nSOURCES:\\n\" + \"\\n\".join(\n",
    "                [f\"- (len={len(getattr(s, 'page_content', ''))}) {getattr(s, 'metadata', {})}\" for s in sources]\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            eval_block\n",
    "            + \"\\n\\nACTION: RETRIEVE_AGAIN\\nNEW_QUERY: \"\n",
    "            + refined_query\n",
    "            + \"\\nREASONING: \"\n",
    "            + reasoning\n",
    "            + \"\\n\\nCorrected Answer (from re-retrieved docs):\\n\"\n",
    "            + final_answer\n",
    "            + sources_text\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "        final_answer, sources = run_qa_chain(qa_chain, question)\n",
    "\n",
    "        sources_text = \"\"\n",
    "        if sources:\n",
    "            sources_text = \"\\n\\nSOURCES:\\n\" + \"\\n\".join(\n",
    "                [f\"- (len={len(getattr(s, 'page_content', ''))}) {getattr(s, 'metadata', {})}\" for s in sources]\n",
    "            )\n",
    "\n",
    "        confidence = decision.get(\"confidence\") or \"medium\"\n",
    "        return (\n",
    "            eval_block\n",
    "            + f\"\\n\\nACTION: PROCEED_WITH_ANSWER\\nCONFIDENCE: {confidence}\\n\\nAnswer:\\n\"\n",
    "            + final_answer\n",
    "            + sources_text\n",
    "        )\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"what is the largest cat?\"\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Running corrective RAG...\\n\")\n",
    "    result = corrective_rag(q, debug=True)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6524d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3986ffd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the largest cat?\n",
      "Running high-accuracy pipeline...\n",
      "\n",
      "INITIAL VERIFICATION: {\n",
      "  \"verdict\": \"ISSUES\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"The largest cat is the Siberian tiger, also known as the Amur tiger.\",\n",
      "      \"status\": \"UNSUPPORTED\",\n",
      "      \"evidence\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"They can weigh up to 660 pounds and grow up to 11 feet in length, including their tail.\",\n",
      "      \"status\": \"UNSUPPORTED\",\n",
      "      \"evidence\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The model answer makes unsupported claims about the Siberian tiger.\"\n",
      "}\n",
      "Verification found issues. Attempt 2 refined query -> What is the largest species of cat in the world?\n",
      "RE-VERIFICATION: {\n",
      "  \"verdict\": \"ISSUES\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"The largest cat is the Siberian tiger, also known as the Amur tiger.\",\n",
      "      \"status\": \"UNSUPPORTED\",\n",
      "      \"evidence\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"They can weigh up to 660 pounds and grow up to 11 feet in length, including their tail.\",\n",
      "      \"status\": \"UNSUPPORTED\",\n",
      "      \"evidence\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The model answer contains unsupported claims about the Siberian tiger.\"\n",
      "}\n",
      "\n",
      "FINAL ANSWER:\n",
      " The largest cat is the Siberian tiger, also known as the Amur tiger. They can weigh up to 660 pounds and grow up to 11 feet in length, including their tail.\n",
      "\n",
      "VERIFICATION:\n",
      " {\n",
      "  \"verdict\": \"ISSUES\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"The largest cat is the Siberian tiger, also known as the Amur tiger.\",\n",
      "      \"status\": \"UNSUPPORTED\",\n",
      "      \"evidence\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"claim\": \"They can weigh up to 660 pounds and grow up to 11 feet in length, including their tail.\",\n",
      "      \"status\": \"UNSUPPORTED\",\n",
      "      \"evidence\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"summary\": \"The model answer contains unsupported claims about the Siberian tiger.\"\n",
      "}\n",
      "\n",
      "SOURCES (first 300 chars each):\n",
      "1. Hello Satheeshkumar Subramanian, GENAI Architect!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load env file if you use .env (optional)\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# ---------------------------\n",
    "# Config / API key handling\n",
    "# ---------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. Please set it in your environment or in a .env file.\\n\\n\"\n",
    "        \"Example .env content:\\n\"\n",
    "        \"  OPENAI_API_KEY=sk-your_api_key_here\\n\"\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# System prompt for corrective evaluation (kept as reference)\n",
    "# Note: braces doubled so .format() won't treat them as placeholders.\n",
    "# ---------------------------\n",
    "CORRECTIVE_RAG_SYSTEM_PROMPT = \"\"\"\n",
    "You are a Corrective RAG system that evaluates retrieved context quality and corrects retrieval when necessary.\n",
    "\n",
    "Primary Workflow:\n",
    "\n",
    "Step 1: Context Evaluation\n",
    "EVALUATE_CONTEXT: Rate the following retrieved context for the given query.\n",
    "\n",
    "Query: {user_query}\n",
    "Retrieved Context: {retrieved_context}\n",
    "\n",
    "Return a JSON object EXACTLY with these fields:\n",
    "{{\n",
    "  \"relevance\": <float 0.0-1.0>,\n",
    "  \"completeness\": <float 0.0-1.0>,\n",
    "  \"accuracy\": <float 0.0-1.0>,\n",
    "  \"specificity\": <float 0.0-1.0>,\n",
    "  \"overall\": \"<EXCELLENT|GOOD|FAIR|POOR>\",\n",
    "  \"justification\": \"<one-sentence justification>\",\n",
    "  \"decision\": {{\n",
    "     \"action\": \"<RETRIEVE_AGAIN|PROCEED_WITH_ANSWER>\",\n",
    "     \"new_query\": \"<refined_query or empty string>\",\n",
    "     \"reasoning\": \"<short reason>\",\n",
    "     \"confidence\": \"<high|medium|low or empty>\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Important:\n",
    "- All numeric scores must be between 0.0 and 1.0.\n",
    "- \"overall\" must be one of: EXCELLENT, GOOD, FAIR, POOR.\n",
    "- Fill \"new_query\" only when action == \"RETRIEVE_AGAIN\" (otherwise empty string).\n",
    "- Return ONLY the JSON object (no extra commentary).\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Load docs and build vectorstore\n",
    "# ---------------------------\n",
    "loader = TextLoader(\"notes.txt\")  # ensure notes.txt exists\n",
    "docs = loader.load()\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# embeddings + FAISS\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# High-k retriever so we can rerank locally\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# LLM (deterministic)\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)\n",
    "\n",
    "# ---------------------------\n",
    "# Utility helpers\n",
    "# ---------------------------\n",
    "def extract_json_substring(s: str) -> str:\n",
    "    brace_stack = []\n",
    "    start_idx = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch == \"{\":\n",
    "            if start_idx is None:\n",
    "                start_idx = i\n",
    "            brace_stack.append(i)\n",
    "        elif ch == \"}\":\n",
    "            if brace_stack:\n",
    "                brace_stack.pop()\n",
    "                if not brace_stack and start_idx is not None:\n",
    "                    return s[start_idx:i + 1]\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    return m.group(0) if m else \"\"\n",
    "\n",
    "def _cosine_sim(a, b):\n",
    "    # expects iterable of floats\n",
    "    dot = sum(x * y for x, y in zip(a, b))\n",
    "    norm_a = math.sqrt(sum(x * x for x in a)) if a else 0.0\n",
    "    norm_b = math.sqrt(sum(y * y for y in b)) if b else 0.0\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0\n",
    "    return dot / (norm_a * norm_b)\n",
    "\n",
    "def run_qa_chain(qa_chain, question: str):\n",
    "    \"\"\"\n",
    "    Safely invoke a QA chain that returns multiple outputs.\n",
    "    Returns tuple: (answer_text, source_documents_list)\n",
    "    \"\"\"\n",
    "    outputs = None\n",
    "    try:\n",
    "        outputs = qa_chain.invoke({\"query\": question})\n",
    "    except Exception:\n",
    "        try:\n",
    "            outputs = qa_chain({\"query\": question})\n",
    "        except Exception:\n",
    "            outputs = None\n",
    "\n",
    "    if not outputs:\n",
    "        return \"\", []\n",
    "\n",
    "    # keys differ across versions; choose best available\n",
    "    answer = outputs.get(\"result\") or outputs.get(\"output_text\") or outputs.get(\"answer\") or outputs.get(\"result_text\") or \"\"\n",
    "    sources = outputs.get(\"source_documents\") or outputs.get(\"source_document\") or outputs.get(\"sources\") or []\n",
    "    return answer, sources\n",
    "\n",
    "# ---------------------------\n",
    "# Reranker: use embeddings cosine similarity\n",
    "# ---------------------------\n",
    "def rerank_docs_by_similarity(query: str, docs: list, top_n: int):\n",
    "    \"\"\"\n",
    "    Return top_n documents from docs sorted by cosine similarity to the query.\n",
    "    \"\"\"\n",
    "    # embed query\n",
    "    try:\n",
    "        q_vec = embeddings.embed_query(query)\n",
    "    except Exception:\n",
    "        q_vec = embeddings.embed_documents([query])[0]\n",
    "\n",
    "    texts = [d.page_content for d in docs]\n",
    "    try:\n",
    "        doc_vecs = embeddings.embed_documents(texts)\n",
    "    except Exception:\n",
    "        # fallback zero vectors\n",
    "        doc_vecs = [[0.0] * len(q_vec) for _ in texts]\n",
    "\n",
    "    sims = [_cosine_sim(q_vec, dv) for dv in doc_vecs]\n",
    "    scored = list(zip(sims, docs))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, doc in scored[:top_n]]\n",
    "\n",
    "# ---------------------------\n",
    "# Query refinement helper\n",
    "# ---------------------------\n",
    "def generate_refined_query(user_query: str, retrieved_context: str) -> str:\n",
    "    \"\"\"\n",
    "    Short keyword style refined query\n",
    "    \"\"\"\n",
    "    refine_prompt = (\n",
    "        \"You are a query-refinement assistant. Produce a short keyword-focused query (one line) \"\n",
    "        \"that will retrieve more relevant documents for the user's intent.\\n\\n\"\n",
    "        f\"User query: {user_query}\\n\\nRetrieved context:\\n{retrieved_context}\\n\\nRefined query:\"\n",
    "    )\n",
    "    try:\n",
    "        out = llm.predict(refine_prompt)\n",
    "        return out.splitlines()[0].strip()\n",
    "    except Exception:\n",
    "        return user_query\n",
    "\n",
    "# ---------------------------\n",
    "# Verifier prompt (returns JSON) - escape braces by doubling\n",
    "# ---------------------------\n",
    "VERIFY_PROMPT = \"\"\"\n",
    "You are an objective verifier. Given a model answer and a set of source snippets, check each factual claim in the answer and determine whether it is:\n",
    "- SUPPORTED (directly backed by at least one source snippet),\n",
    "- PARTIALLY_SUPPORTED (some support but not precise),\n",
    "- UNSUPPORTED (no support),\n",
    "- CONTRADICTED (source contradicts claim).\n",
    "\n",
    "Return a JSON object exactly like:\n",
    "{{\n",
    "  \"verdict\": \"<OK|ISSUES>\",\n",
    "  \"claims\": [\n",
    "    {{\"claim\": \"<short text of claim>\", \"status\": \"<SUPPORTED|PARTIALLY_SUPPORTED|UNSUPPORTED|CONTRADICTED>\", \"evidence\": \"<which snippet index(es) or empty>\"}}\n",
    "  ],\n",
    "  \"summary\": \"<one-sentence summary>\"\n",
    "}}\n",
    "\n",
    "Model answer:\n",
    "\\\"\\\"\\\"{answer}\\\"\\\"\\\"\n",
    "\n",
    "Source snippets (numbered): \n",
    "{snippets}\n",
    "Return only the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "def verify_answer_against_sources(answer: str, snippets: list) -> dict:\n",
    "    \"\"\"\n",
    "    Ask the LLM to verify the answer against the snippets; returns parsed JSON with claims statuses.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for i, s in enumerate(snippets):\n",
    "        # truncate snippet and remove double quotes safely\n",
    "        clean = s[:800].replace('\"', '')\n",
    "        processed.append(f\"{i+1}. {clean}\")\n",
    "    numbered = \"\\n\".join(processed)\n",
    "\n",
    "    prompt = VERIFY_PROMPT.format(answer=answer.replace('\"', \"'\"), snippets=numbered)\n",
    "    try:\n",
    "        raw = llm.predict(prompt)\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"verdict\": \"ISSUES\",\n",
    "            \"claims\": [],\n",
    "            \"summary\": f\"Verification failed: {e}\",\n",
    "            \"raw\": str(e)\n",
    "        }\n",
    "\n",
    "    # parse JSON robustly\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except Exception:\n",
    "        candidate = extract_json_substring(raw)\n",
    "        try:\n",
    "            parsed = json.loads(candidate) if candidate else {\n",
    "                \"verdict\": \"ISSUES\",\n",
    "                \"claims\": [],\n",
    "                \"summary\": \"Unable to parse verifier output\",\n",
    "                \"raw\": raw\n",
    "            }\n",
    "        except Exception:\n",
    "            parsed = {\n",
    "                \"verdict\": \"ISSUES\",\n",
    "                \"claims\": [],\n",
    "                \"summary\": \"Unable to parse verifier output\",\n",
    "                \"raw\": raw\n",
    "            }\n",
    "    return parsed\n",
    "\n",
    "# ---------------------------\n",
    "# High-accuracy answer routine (rerank + QA + verify + optional retry)\n",
    "# ---------------------------\n",
    "TOP_K_FOR_ANSWER = 5\n",
    "MAX_VERIFICATION_ATTEMPTS = 2\n",
    "\n",
    "def answer_with_verification(question: str, max_attempts: int = MAX_VERIFICATION_ATTEMPTS, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Retrieve, rerank, answer, verify and optionally re-retrieve & re-answer if verification fails.\n",
    "    Returns (final_answer, verification_result, used_snippets)\n",
    "    \"\"\"\n",
    "    # initial retrieval\n",
    "    candidates = retriever.get_relevant_documents(question)\n",
    "    if not candidates:\n",
    "        return \"No documents retrieved\", {\"verdict\": \"ISSUES\", \"claims\": [], \"summary\": \"No candidates\"}, []\n",
    "\n",
    "    # rerank locally and pick top_k\n",
    "    top_docs = rerank_docs_by_similarity(question, candidates, top_n=TOP_K_FOR_ANSWER)\n",
    "    snippets = [d.page_content for d in top_docs]\n",
    "    context_for_prompt = \"\\n\\n---\\n\\n\".join(snippets[:TOP_K_FOR_ANSWER])\n",
    "\n",
    "    # build a temporary FAISS for the top docs and run QA\n",
    "    db = FAISS.from_documents(top_docs, embeddings)\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(), return_source_documents=True)\n",
    "    final_answer, sources = run_qa_chain(qa_chain, question)\n",
    "\n",
    "    verification = verify_answer_against_sources(final_answer, snippets)\n",
    "    if debug:\n",
    "        print(\"INITIAL VERIFICATION:\", json.dumps(verification, indent=2))\n",
    "\n",
    "    attempts = 1\n",
    "    while attempts < max_attempts and verification.get(\"verdict\", \"ISSUES\") != \"OK\":\n",
    "        # refine query using verification summary\n",
    "        refine_hint = verification.get(\"summary\") or \"find supporting evidence for claims\"\n",
    "        refined_q = generate_refined_query(question + \" \" + refine_hint, context_for_prompt)\n",
    "        if debug:\n",
    "            print(f\"Verification found issues. Attempt {attempts+1} refined query ->\", refined_q)\n",
    "        candidates = retriever.get_relevant_documents(refined_q)\n",
    "        if not candidates:\n",
    "            break\n",
    "        top_docs = rerank_docs_by_similarity(refined_q, candidates, top_n=TOP_K_FOR_ANSWER)\n",
    "        snippets = [d.page_content for d in top_docs]\n",
    "        db = FAISS.from_documents(top_docs, embeddings)\n",
    "        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(), return_source_documents=True)\n",
    "        final_answer, sources = run_qa_chain(qa_chain, question)\n",
    "        verification = verify_answer_against_sources(final_answer, snippets)\n",
    "        if debug:\n",
    "            print(\"RE-VERIFICATION:\", json.dumps(verification, indent=2))\n",
    "        attempts += 1\n",
    "\n",
    "    return final_answer, verification, snippets\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"what is the largest cat?\"\n",
    "    print(\"Question:\", q)\n",
    "    print(\"Running high-accuracy pipeline...\\n\")\n",
    "    ans, verification, used_snippets = answer_with_verification(q, max_attempts=2, debug=True)\n",
    "\n",
    "    print(\"\\nFINAL ANSWER:\\n\", ans)\n",
    "    print(\"\\nVERIFICATION:\\n\", json.dumps(verification, indent=2))\n",
    "    if used_snippets:\n",
    "        print(\"\\nSOURCES (first 300 chars each):\")\n",
    "        for i, s in enumerate(used_snippets):\n",
    "            # preprocess to avoid backslashes in f-string expression\n",
    "            snippet_preview = s[:300].replace(\"\\n\", \" \")\n",
    "            print(f\"{i+1}. {snippet_preview}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bca33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs =[\n",
    "       Document(page_content =\"The largest cat is a tiger.\"),\n",
    "       Document(page_content =\"The largest dog is a great dane.\"),\n",
    "       Document(page_content =\"The largest animal is a blue whale.\")\n",
    "       ]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db =FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "fusion_rag_chain = RetrievalQA.from_chain_type(llm=llm , retriever =db.as_retriever(),return_source_documents=False)\n",
    "\n",
    "print(fusion_rag_chain.run(\"what is the¬†largest¬†cat?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e5de61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61206574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc34182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-win_amd64.whl (18.2 MB)\n",
      "     ---------------------------------------- 18.2/18.2 MB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b1aaea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-win_amd64.whl (884 kB)\n",
      "     -------------------------------------- 884.4/884.4 kB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8616f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.108.0-py3-none-any.whl (948 kB)\n",
      "     -------------------------------------- 948.1/948.1 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.11.0-cp311-cp311-win_amd64.whl (204 kB)\n",
      "     -------------------------------------- 204.3/204.3 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.11.0 openai-1.108.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a89bc442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'title': 'sample', 'author': 'Philip Hutchison', 'moddate': \"D:20080701052447Z00'00'\", 'source': 'sample.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Sample PDFThis is a simple PDF Ô¨Åle. Fun fun fun.\\nLorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \\nCurabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \\npharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \\nInteger a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. \\nVestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla \\nerat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \\nVivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \\naccumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, \\nhendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. \\nSuspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.\\nPellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam \\nlobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. \\nPhasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh \\nante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis \\nsem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin \\nvelit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, \\nfeugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat \\nfeugiat. Aenean pellentesque.\\nIn mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat \\nvarius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem \\npede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce \\nconsectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce \\nvulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non \\nnibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. \\nSuspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae \\nenim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.\\nMorbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque \\npenatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit \\namet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa \\neget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, \\nquam.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Satheesh\\GenAI - agentflow\\Pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697f4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Satheesh\\GenAI - agentflow\\Pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Satheesh\\GenAI - agentflow\\Pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jsath\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47e5631",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chroma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m texts =[\u001b[33m\"\u001b[39m\u001b[33mThe quick brows fox jumps over the lazy dog\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m      2\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mLangchain makes it easy to work with LLM\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mchroma is another vector store similar to FAISS\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m.from_texts(texts,embedding = embeddings)\n",
      "\u001b[31mNameError\u001b[39m: name 'Chroma' is not defined"
     ]
    }
   ],
   "source": [
    "texts =[\"The quick brows fox jumps over the lazy dog\", \n",
    "\"Langchain makes it easy to work with LLM\",\n",
    "\"chroma is another vector store similar to FAISS\"]\n",
    "vectorstore = Chroma.from_texts(texts,embedding = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914ea450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (1.0.21)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (2.11.9)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (2.3.2)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (4.15.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (1.75.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (0.17.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: packaging>=19.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: anyio in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
      "Requirement already satisfied: certifi in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.5)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.0)\n",
      "Requirement already satisfied: sympy in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.34.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: filelock in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6c2af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-huggingface in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (0.3.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "     -------------------------------------- 483.4/483.4 kB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-huggingface) (0.3.76)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-huggingface) (0.22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-huggingface) (0.34.6)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "     ---------------------------------------- 11.6/11.6 MB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0\n",
      "  Downloading torch-2.8.0-cp311-cp311-win_amd64.whl (241.4 MB)\n",
      "     -------------------------------------- 241.4/241.4 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: Pillow in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 8.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2025.9.1-cp311-cp311-win_amd64.whl (276 kB)\n",
      "     -------------------------------------- 276.2/276.2 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "     -------------------------------------- 320.2/320.2 kB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2025.8.3)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     -------------------------------------- 536.2/536.2 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: anyio in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
      "Installing collected packages: mpmath, sympy, safetensors, regex, networkx, torch, transformers, sentence-transformers\n",
      "Successfully installed mpmath-1.3.0 networkx-3.5 regex-2025.9.1 safetensors-0.6.2 sentence-transformers-5.1.0 sympy-1.14.0 torch-2.8.0 transformers-4.56.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98db300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-huggingface) (0.3.76)\n",
      "Collecting tokenizers>=0.19.1\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.33.4\n",
      "  Downloading huggingface_hub-0.34.6-py3-none-any.whl (562 kB)\n",
      "     -------------------------------------- 562.6/562.6 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "     ------------------------------------- 199.3/199.3 kB 11.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.5)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.15.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: colorama in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.33.4->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: anyio in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\satheesh\\genai - agentflow\\pyautogui\\pythonfundamentals\\pyautogui\\pyautogui\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n",
      "Installing collected packages: tqdm, fsspec, filelock, huggingface-hub, tokenizers, langchain-huggingface\n",
      "Successfully installed filelock-3.19.1 fsspec-2025.9.0 huggingface-hub-0.34.6 langchain-huggingface-0.3.1 tokenizers-0.22.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c822091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f24e379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'notes.txt'}, page_content='Hello Satheeshkumar Subramanian, GENAI Architect!')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ed48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TextLoader('notes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86525307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31dcacec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
      "     -------------------------------------- 310.5/310.5 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogui (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
